{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FiEfq3H0lV-"
      },
      "source": [
        "In this practical session we will see how to compute documents embeddings and use them for text classification.\n",
        "\n",
        "\n",
        "1.  In the first part we will use bag of words methods\n",
        "2.  In the second part we will explore word embeddings methods\n",
        "3. In the last part you will play with transformers\n",
        "\n",
        "During all the practical session, you will work on the same dataset [AG's corpus of news article] which will help you to fairly compare each approches.\n",
        "\n",
        "*AG News (AG’s News Corpus) is a subdataset of [AG's corpus](http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html) of news articles constructed by assembling titles and description fields of articles from the 4 largest classes (“World”, “Sports”, “Business”, “Sci/Tech”) of AG’s Corpus. The AG News contains 30,000 training and 1,900 test samples per class.*  \n",
        "\n",
        "Let's first download the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pHxFAD3N0336"
      },
      "outputs": [],
      "source": [
        "# !wget https://github.com/mhjabreel/CharCnn_Keras/raw/master/data/ag_news_csv/train.csv 2>&1\n",
        "# !wget https://github.com/mhjabreel/CharCnn_Keras/raw/master/data/ag_news_csv/test.csv 2>&1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXGIafRr088b"
      },
      "source": [
        "The following code snippet will load the dataset and incorporate the label names into a new column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSF075r90-UA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "traindf = pd.read_csv('train.csv', names=[\"label\", \"title\", \"text\"]).sample(2000)\n",
        "testdf = pd.read_csv('test.csv', names=[\"label\", \"title\", \"text\"]).sample(1000)\n",
        "\n",
        "traindf['label'] = traindf['label'] -1\n",
        "traindf['label_name'] = traindf.label.map({0:\"World\", 1:\"Sports\", 2:\"Business\", 3:\"Sci/Tech\"})\n",
        "testdf['label'] = testdf['label'] -1\n",
        "testdf['label_name'] = testdf.label.map({0:\"World\", 1:\"Sports\", 2:\"Business\", 3:\"Sci/Tech\"})\n",
        "testdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKhkuBBO1EJO"
      },
      "source": [
        "\n",
        "**Begin with Initial Exploration:**\n",
        "\n",
        "To kick off our analysis, let's start with a bit of exploration.  \n",
        "Use the [Wordcloud](https://amueller.github.io/word_cloud/) library and follow the example provided in the [documentation](https://amueller.github.io/word_cloud/auto_examples/simple.html#sphx-glr-auto-examples-simple-py) to create a visualization of the most common words in our corpus.\n",
        "\n",
        "To get started, you can concatenate all the documents into a single text. A straightforward way to do this is by using the `\"\".join()` function on a list of text...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jP1jEhbv1Dhv"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "corpus = \" \".join(traindf.text.values)\n",
        "wordcloud = WordCloud().generate(corpus)\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.show();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRY-cCwW1J8L"
      },
      "source": [
        "By default, Wordcloud automatically excludes common English words such as \"and,\" \"or,\" \"the,\" and \"a.\" However, you can customize this list of excluded words to suit your needs.\n",
        "\n",
        "If you're looking for a powerful library for natural language processing, consider using [NLTK](https://www.nltk.org/). NLTK provides several lists of stopwords that can be used to clean text data.\n",
        "\n",
        "Even though using NLTK stopwords may not significantly alter the results, let's go ahead and provide Wordcloud with a custom list of stopwords sourced from NLTK.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KvDVZLPy1K0A"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "wordcloud = WordCloud(stopwords=stop_words).generate(corpus) #use the stopwords when generating the wordcloud\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.show();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmdvhkIC1Shg"
      },
      "source": [
        "Now, plot a different wordcloud for every category in the dataset.  \n",
        "Are you capable of predicting the categories given only these wordclouds?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8OxBBVFm1Q7s"
      },
      "outputs": [],
      "source": [
        "groupby = traindf.groupby(by=\"label_name\")\n",
        "for x in groupby:\n",
        "    print(x[0])\n",
        "    mini_corpus = \" \".join(x[1].text.values)\n",
        "    wordcloud = WordCloud(stopwords=stop_words).generate(mini_corpus) #use the stopwords when generating the wordcloud\n",
        "    plt.imshow(wordcloud)\n",
        "    plt.axis(\"off\")\n",
        "    plt.show();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1gUdpfd1amf"
      },
      "source": [
        "# Bag-of-Words (BoW)\n",
        "\n",
        "In this section, we will train various models to predict the category of news articles. One of the initial approaches we'll explore is called the \"**bag of words**\" (BoW) method.\n",
        "\n",
        "BoW methods represent documents by counting or calculating statistics on the words present within them. Once the bag-of-words is computed, documents are transformed into vectors, with each dimension corresponding to a word/token in the vocabulary of the corpus.\n",
        "\n",
        "To start, vectorize your documents using term frequencies.   \n",
        "You can refer to the [documentation of scikit-learn's CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) to encode the **text column** of your training set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7ikZdT-1crg"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "train_count_matrix = vectorizer.fit_transform(traindf.text.values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtcbMNuy1f0j"
      },
      "source": [
        "The ```vocabulary_``` argument of your vectorizer contains a dictionary with all the tokens and their corresponding index in the bag-of-words.  \n",
        "How many unique tokens compose your bag-of-words?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6cdLuvP1iWy"
      },
      "outputs": [],
      "source": [
        "len(vectorizer.vocabulary_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ou2LkG21sUr"
      },
      "source": [
        "You can also use the ```get_feature_names_out()``` method to get the list of identified tokens:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uggH0vvn1qV8"
      },
      "outputs": [],
      "source": [
        "vectorizer.get_feature_names_out()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Altpzkw1mgi"
      },
      "source": [
        "Now choose a classification method from scikit-learn and train it to classify news article.  \n",
        "Print the classification score of your model on the training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r0NXT1zs11cb"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "classifier = RandomForestClassifier()\n",
        "classifier.fit(train_count_matrix, traindf.label_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YR7Ou2Qj145D"
      },
      "source": [
        "Now use the ```transform``` method from your vectorizer on the testing set and print the score obtained by your model on the testing set.  \n",
        "Your model is probably overfitting a lot.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqUq_yFw14dL"
      },
      "outputs": [],
      "source": [
        "test_count_matrix = vectorizer.transform(testdf.text)\n",
        "classifier.score(test_count_matrix, testdf.label_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5eoFbB8fo08"
      },
      "source": [
        "\n",
        "Plot a [consusion matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html) to see where your model makes the most mistakes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgvxh_HP14YW"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "cm = confusion_matrix(classifier.predict(test_count_matrix), testdf.label_name)\n",
        "ConfusionMatrixDisplay(cm, display_labels=classifier.classes_).plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcZ5any32BjD"
      },
      "source": [
        "Play with some of the vectorizer hyper-parameters to see whether you can improve the perfomance of your classifier on the testing set.  \n",
        "Try adding stopwords or changing the ngram_range..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bIqzoSax2FBS"
      },
      "outputs": [],
      "source": [
        "vectorizer = CountVectorizer(stop_words=\"english\", ngram_range=(1, 2))\n",
        "train_count_matrix = vectorizer.fit_transform(traindf.text.values)\n",
        "classifier = RandomForestClassifier()\n",
        "classifier.fit(train_count_matrix, traindf.label_name)\n",
        "test_count_matrix = vectorizer.transform(testdf.text)\n",
        "print(classifier.score(test_count_matrix, testdf.label_name))\n",
        "cm = confusion_matrix(classifier.predict(test_count_matrix), testdf.label_name)\n",
        "ConfusionMatrixDisplay(cm, display_labels=classifier.classes_).plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVcA6OOR2JFQ"
      },
      "source": [
        "Once you are satisfied with the model's performance or find that it's not improving further, you can proceed to plot a [t-SNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html#sklearn.manifold.TSNE) visualization of your training representations, using different colors to represent labels.\n",
        "\n",
        "In particular, compare the t-SNE representations computed with and without stopwords. What observations do you make from this comparison?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uvs9ha_j2a8U"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "import seaborn as sns\n",
        "\n",
        "# fist a t-sne of representations computed without using stopwords\n",
        "# use the labels as hue\n",
        "vectorizer = CountVectorizer()\n",
        "vectors = vectorizer.fit_transform(traindf.text.values)\n",
        "tsne = TSNE(init=\"random\")\n",
        "projection = tsne.fit_transform(vectors)\n",
        "projection_df = pd.DataFrame()\n",
        "projection_df[\"x\"] = projection[:, 0]\n",
        "projection_df[\"y\"] = projection[:, 1]\n",
        "projection_df[\"label\"] = traindf.label_name.values\n",
        "print(projection_df.shape)\n",
        "sns.scatterplot(projection_df, x=\"x\", y=\"y\", hue=\"label\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# then a t-sne of representations computed using stopwords\n",
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "vectors = vectorizer.fit_transform(traindf.text.values)\n",
        "tsne = TSNE(init=\"random\")\n",
        "projection = tsne.fit_transform(vectors)\n",
        "projection_df = pd.DataFrame()\n",
        "projection_df[\"x\"] = projection[:, 0]\n",
        "projection_df[\"y\"] = projection[:, 1]\n",
        "projection_df[\"label\"] = traindf.label_name.values\n",
        "print(projection_df.shape)\n",
        "sns.scatterplot(projection_df, x=\"x\", y=\"y\", hue=\"label\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "447q9XfE2eH7"
      },
      "source": [
        "Now, let's explore a second vectorization strategy, which is more efficient than pure term frequency: TF-IDF (Term Frequency-Inverse Document Frequency).\n",
        "\n",
        "**What's the difference compared to the previous method?**\n",
        "\n",
        "TF-IDF takes into account not only the frequency of terms in a document but also their importance in the entire corpus. It assigns higher weights to terms that are frequent in a document but rare across all documents. This helps in distinguishing important terms from common ones.\n",
        "\n",
        "To implement TF-IDF vectorization, you can use [scikit-learn's `TfidfVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer). Once you've vectorized the documents, train a classification algorithm to classify them. Afterward, print the score obtained on the testing set and the corresponding confusion matrix.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3m3yMfr82f7S"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
        "vectors = vectorizer.fit_transform(traindf.text.values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g68lyvtd2iCV"
      },
      "source": [
        "Plot a t-SNE of the representations obtained using TF-IDF."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4hTbfN_V2kHT"
      },
      "outputs": [],
      "source": [
        "tsne = TSNE(n_components=2, init='random')\n",
        "projection = tsne.fit_transform(vectors)\n",
        "projection_df = pd.DataFrame()\n",
        "projection_df[\"x\"] = projection[:, 0]\n",
        "projection_df[\"y\"] = projection[:, 1]\n",
        "projection_df[\"label\"] = traindf.label_name.values\n",
        "sns.scatterplot(projection_df, x=\"x\", y=\"y\", hue=\"label\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrvsDhos2m2X"
      },
      "source": [
        "Both the ```TfidfVectorizer``` and ```CountVectorizer``` use a default strategy to create a token given a text using whitespaces and punctuations as separators.  \n",
        "It is possible to provide custom __tokenizers__ to these vectorizers.  \n",
        "Here we will use NLTK to build a more powerful tokenizer that will:\n",
        "\n",
        "*   Revmove stop words\n",
        "*   Convert all texts to lowercase\n",
        "*   Ignore punctuations symbols\n",
        "*   Only consider letters\n",
        "*   Perform Stemming on every token\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZsCaGW22pAI"
      },
      "outputs": [],
      "source": [
        "from nltk import word_tokenize\n",
        "from nltk.stem import SnowballStemmer\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "# Download stopwords list\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Interface lemma tokenizer from nltk with sklearn\n",
        "class StemTokenizer:\n",
        "    ignore_tokens = [',', '.', ';', ':', '\"', '``', \"''\", '`']\n",
        "    def __init__(self):\n",
        "        self.stemmer = SnowballStemmer('english')\n",
        "    def __call__(self, doc):\n",
        "        doc = doc.lower()\n",
        "        return [self.stemmer.stem(t) for t in word_tokenize(re.sub(\"[^a-z' ]\", \"\", doc)) if t not in self.ignore_tokens]\n",
        "\n",
        "tokenizer=StemTokenizer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lZM9m8B2rdb"
      },
      "source": [
        "Print an example of text from the dataset and the corresponding tokens computed by the tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79uiRnTx2tQ3"
      },
      "outputs": [],
      "source": [
        "text = traindf.text.values[0][:100]\n",
        "print(text)\n",
        "print(tokenizer(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiSlM6jw2u_R"
      },
      "source": [
        "Now provide the tokenizer to the a ```TfidfVectorizer``` and repeat the entire process.  \n",
        "Does it improves the testing performance?  \n",
        "Tips: you should also provide a tokenized version of the stopwords since we apply stemming on all tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1OmGn4F62we_"
      },
      "outputs": [],
      "source": [
        "token_stop = tokenizer(' '.join(stop_words))\n",
        "\n",
        "vectorizer = TfidfVectorizer(stop_words=token_stop, tokenizer=tokenizer)\n",
        "vectors = vectorizer.fit_transform(traindf.text.values)\n",
        "tsne = TSNE(n_components=2, init='random')\n",
        "projection = tsne.fit_transform(vectors)\n",
        "projection_df = pd.DataFrame()\n",
        "projection_df[\"x\"] = projection[:, 0]\n",
        "projection_df[\"y\"] = projection[:, 1]\n",
        "projection_df[\"label\"] = traindf.label_name.values\n",
        "sns.scatterplot(projection_df, x=\"x\", y=\"y\", hue=\"label\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBZ7tr6V22LT"
      },
      "source": [
        "It is also possible to combine bag-of-words features with other features manually computed.  \n",
        "The following code computes some new features on all documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9JNM7yKH24ZB"
      },
      "outputs": [],
      "source": [
        "def count_chars(text):\n",
        "    return len(text)\n",
        "\n",
        "def count_words(text):\n",
        "    return len(text.split())\n",
        "\n",
        "def count_capital_words(string):\n",
        "    return sum(map(str.isupper, string))\n",
        "\n",
        "def count_capital_words(text):\n",
        "  return sum(map(str.isupper,text.split()))\n",
        "\n",
        "def count_punctuations(text):\n",
        "  count = 0\n",
        "  for i in range (0, len (text)):\n",
        "    if text[i] in ('!', \",\" ,\"\\'\" ,\";\" ,\"\\\"\", \".\", \"-\" ,\"?\"):\n",
        "        count = count + 1;\n",
        "  return  count\n",
        "\n",
        "def count_sentences(text):\n",
        "    return len(nltk.sent_tokenize(text))\n",
        "\n",
        "def count_unique_words(text):\n",
        "    return len(set(text.split()))\n",
        "\n",
        "for df in [traindf, testdf]:\n",
        "  df['count_chars'] = df.text.apply(lambda s: count_chars(s))\n",
        "  df['count_words'] = df.text.apply(lambda s: count_words(s))\n",
        "  df['count_capital_words'] = df.text.apply(lambda s: count_capital_words(s))\n",
        "  df['count_capital_words'] = df.text.apply(lambda s: count_capital_words(s))\n",
        "  df['count_punctuations'] = df.text.apply(lambda s: count_punctuations(s))\n",
        "  df['count_sentences'] = df.text.apply(lambda s: count_sentences(s))\n",
        "  df['count_unique_words'] = df.text.apply(lambda s: count_unique_words(s))\n",
        "  df['avg_wordlength'] = df['count_chars']/df['count_words']\n",
        "  df['avg_sentlength'] = df['count_words']/df['count_sentences']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Su4p1kmP27S0"
      },
      "source": [
        "Using a [```ColumnTransformer```](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html#sklearn.compose.ColumnTransformer) it is possible to combine all the features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FATcH94G261y"
      },
      "outputs": [],
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "columns_to_keep = ['text', 'count_chars', 'count_words',\n",
        "       'count_capital_words', 'count_punctuations',\n",
        "       'count_unique_words', 'count_sentences', 'avg_wordlength',\n",
        "       'avg_sentlength']\n",
        "\n",
        "column_trans = ColumnTransformer(\n",
        "    [('categories', TfidfVectorizer(stop_words=token_stop, tokenizer=tokenizer), 'text')],\n",
        "    remainder='passthrough', verbose_feature_names_out=False)\n",
        "\n",
        "X_train = column_trans.fit_transform(traindf[columns_to_keep])\n",
        "X_test = column_trans.transform(testdf[columns_to_keep])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PW7Xh8lV2-1S"
      },
      "source": [
        "Unfortunately, in our particular case, these features do not seem to lead to any improvement in testing performance.  \n",
        "However, it's important to note that in different tasks, such as spam detection, these features can have a more pronounced and positive impact.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQ_3Gw9n3Blk"
      },
      "outputs": [],
      "source": [
        "rf = RandomForestClassifier()\n",
        "rf.fit(X_train, traindf.label)\n",
        "print(f\"Model score on training data: {rf.score(X_train, traindf.label):.2f}\")\n",
        "print(f\"Model score on test data: {rf.score(X_test, testdf.label)}\")\n",
        "\n",
        "predictions = rf.predict(X_test)\n",
        "cm = confusion_matrix(testdf.label, predictions)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"World\", \"Sports\", \"Business\", \"Sci/Tech\"])\n",
        "disp.plot()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JraoOnls3dHJ"
      },
      "source": [
        "# Word Vectorization with Gensim and GloVe\n",
        "\n",
        "In this section, we will explore another vectorization technique introduced during our course lectures: word vectorization. Specifically, we will use the Gensim library to work with pre-computed word embeddings.\n",
        "\n",
        "To begin, we'll utilize pre-computed word embeddings called [GloVe](https://nlp.stanford.edu/projects/glove/). GloVe, which stands for \"Global Vectors for Word Representation,\" is an innovative method for generating word embeddings.\n",
        "\n",
        "GloVe embeddings are unique because they capture not only the local context of words within sentences but also the global context of words across the entire corpus. This is achieved through a specific training objective.\n",
        "\n",
        "During the training of GloVe embeddings, the model aims to learn word representations in such a way that the dot product between the vectors of two words corresponds to the logarithm of their co-occurrence probability in the corpus. In other words, GloVe embeddings optimize to represent words in a way that reflects how often they appear together in the same context. This results in vector representations that capture meaningful semantic relationships between words.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfRs84Fu3ptV"
      },
      "outputs": [],
      "source": [
        "# !wget https://nlp.stanford.edu/data/glove.6B.zip\n",
        "# !unzip glove.6B.zip > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_iDcAoM3mUJ"
      },
      "outputs": [],
      "source": [
        "from gensim.test.utils import datapath, get_tmpfile\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "glove_file = ('glove.6B.100d.txt')\n",
        "word2vec_glove_file = get_tmpfile(\"glove.6B.100d.word2vec.txt\")\n",
        "glove2word2vec(glove_file, word2vec_glove_file)\n",
        "model = KeyedVectors.load_word2vec_format(word2vec_glove_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GDyfE0w5s4a"
      },
      "source": [
        "The model is a mapping between words and their vector representations.  \n",
        "Here is an exemple of representation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXkb1yQt5u0r"
      },
      "outputs": [],
      "source": [
        "model['apple']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgxSCo015xja"
      },
      "source": [
        "It also has usefull methods to explore the  vocabulary's embeddings.  \n",
        "Here are some examples to find the most similar words in the embedding space.   \n",
        "Try with some other words and look if the most similar words seem plausibles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2gS68nO050Pw"
      },
      "outputs": [],
      "source": [
        "# model.most_similar('zuckerberg')\n",
        "# model.most_similar('google')\n",
        "# model.most_similar('intelligence')\n",
        "model.most_similar(negative='network')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yt5FnWTo6CPg"
      },
      "source": [
        "An other cool feature of Word2Vec is the possibility to perform analogies.  \n",
        "The most famous example is certainly king - man + woman = queen.  \n",
        "Try to find other working analogies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hqUEdNEG6GNS"
      },
      "outputs": [],
      "source": [
        "result = model.most_similar(positive=['woman', 'king'], negative=['man'])\n",
        "# result = model.most_similar(positive=['paris', 'spain'], negative=['france'])\n",
        "print(\"{}: {:.4f}\".format(*result[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7_cl1FJY6NaG"
      },
      "outputs": [],
      "source": [
        "result = model.most_similar(positive=['beer', 'france'], negative=['usa'])\n",
        "for i in range(3):\n",
        "  print(\"{}: {:.4f}\".format(*result[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ku3as-7_6VSX"
      },
      "source": [
        "The following code plots a PCA or t-SNE representation of a list of words.  \n",
        "Use this method with your own list of words to see wether similar words are close to each other in the embedding space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-hBf2DB56WuX"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_embeddings(model, words, reduction='pca'):\n",
        "    word_vectors = np.array([model[w] for w in words])\n",
        "    if reduction == PCA:\n",
        "      reductor = PCA(n_components=2)\n",
        "    elif reduction == \"tsne\":\n",
        "      reductor = TSNE(2, perplexity=20)\n",
        "    X = reductor.fit_transform(word_vectors)\n",
        "    plt.figure(figsize=(12,12))\n",
        "    plt.scatter(X[:,0], X[:,1])\n",
        "    for word, x in zip(words, X):\n",
        "        plt.text(x[0]+0.05, x[1]+0.05, word)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p2m2oYKg6YHP"
      },
      "outputs": [],
      "source": [
        "word_list = ['coffee', 'tea', 'beer', 'wine', 'brandy', 'rum', 'champagne', 'water',\n",
        "                         'spaghetti', 'borscht', 'hamburger', 'pizza', 'falafel', 'sushi', 'meatballs',\n",
        "                         'dog', 'horse', 'cat', 'monkey', 'parrot', 'koala', 'lizard',\n",
        "                         'frog', 'toad', 'monkey', 'ape', 'kangaroo', 'wombat', 'wolf',\n",
        "                         'france', 'germany', 'hungary', 'luxembourg', 'australia', 'fiji', 'china',\n",
        "                         'homework', 'assignment', 'problem', 'exam', 'test', 'class',\n",
        "                         'school', 'college', 'university', 'institute']\n",
        "\n",
        "plot_embeddings(model, words=word_list, reduction='tsne')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9K7Wh81n6dmG"
      },
      "source": [
        "We will now use these pre-computed embeddings to build the document representations.  \n",
        "A simple way to compute a document representation from word embeddings consists in computing the mean or the sum of all the document's word embeddings.  \n",
        "Here, since the documents do not have the same length, it is preferable to use the mean.  \n",
        "Fill in the following code to compute the mean embeddings of all documents.  \n",
        "Since this process is a little bit long, we will use a limited amount of documents during the practical session. Nonetheless, feel free to try with the complete dataset at home.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9-V0r_w6fgX"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "tqdm.pandas()\n",
        "\n",
        "traindf = pd.read_csv('train.csv', names=[\"label\", \"title\", \"text\"]).sample(1000)\n",
        "testdf = pd.read_csv('test.csv', names=[\"label\", \"title\", \"text\"]).sample(200)\n",
        "\n",
        "traindf['label'] = traindf['label'] -1\n",
        "traindf['label_name'] = traindf.label.map({0:\"World\", 1:\"Sports\", 2:\"Business\", 3:\"Sci/Tech\"})\n",
        "testdf['label'] = testdf['label'] -1\n",
        "testdf['label_name'] = testdf.label.map({0:\"World\", 1:\"Sports\", 2:\"Business\", 3:\"Sci/Tech\"})\n",
        "\n",
        "def compute_mean_embeddings(text, model, words_list, dim=100):\n",
        "  text = text.lower()# convert in lower case\n",
        "  emb_list = [model[w] for w in text if w in words_list] # check if the word is in the vocabulary\n",
        "  return sum(emb_list) / len(emb_list)\n",
        "\n",
        "words_list = model.index_to_key\n",
        "traindf['mean_embeddings'] = traindf.text.progress_apply(lambda s: compute_mean_embeddings(s, model, words_list))\n",
        "testdf['mean_embeddings'] = testdf.text.progress_apply(lambda s: compute_mean_embeddings(s, model, words_list))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMuqLFnx6ofe"
      },
      "source": [
        "The following code extracts the computed embeddings from the dataframe.  \n",
        "Use these to train a model to predict the article category.  \n",
        "Print your testing performance and plot a confusion matrix.  \n",
        "The results may be a little bit disappointing. Any idea why?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32T1WxI36vmH"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "X_train = np.vstack(traindf['mean_embeddings'].values)\n",
        "X_test = np.vstack(testdf['mean_embeddings'].values)\n",
        "\n",
        "classifier = RandomForestClassifier()\n",
        "classifier.fit(X_train, traindf[\"label\"])\n",
        "print(classifier.score(X_test, testdf[\"label\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHJxiQlO61Qb"
      },
      "source": [
        "Plot a t-SNE of the computed embeddings.  \n",
        "Does it look like good representations to classify documents?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbtcLn-Z60xS"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "tsne = TSNE(n_components=2, init='pca')\n",
        "projection = tsne.fit_transform(X_train)\n",
        "projection_df = pd.DataFrame()\n",
        "projection_df[\"x\"] = projection[:, 0]\n",
        "projection_df[\"y\"] = projection[:, 1]\n",
        "projection_df[\"label\"] = traindf.label_name.values\n",
        "print(projection_df.shape)\n",
        "sns.scatterplot(projection_df, x=\"x\", y=\"y\", hue=\"label\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6GtK4FR7VZ8"
      },
      "source": [
        "Word2Vec is an self-supervised learning of words represenations. Thus all words representations are meaningfull and have an equal impact when computing the mean.  This means that category irrelevant words have an equal importance in the document average represenatation than other words more related to the category.  \n",
        "Computing the average of word embeddings learned with self-supervised learning is not very efficient for document classification.  \n",
        "In the following we will see two alternatives using deep neural networks:\n",
        "\n",
        "\n",
        "1.   Learn our word embeddings at the same time as we learn the classification function\n",
        "2.   Replace the mean by a recurrent layer responsible for filtering informative words within the sequence\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mnP6doJ0-1h"
      },
      "source": [
        "In this section, we will harness the flexibility of PyTorch to work with word embeddings. First, we'll proceed by downloading the dataset and preparing it to be compatible with PyTorch.\n",
        "\n",
        "Using PyTorch for embeddings provides us with greater flexibility and control over the embedding process. This allows us to tailor embeddings to our specific needs and integrate them seamlessly into deep learning models for various natural language processing tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wMzug4wt8NP0"
      },
      "outputs": [],
      "source": [
        "# %pip install \"portalocker>=2.0.0\" 2>&1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttCDKokk8Vb_"
      },
      "source": [
        "You may need to restart your runtime at this point if the next cell returns an error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rsqx9bhg08iP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchtext.datasets import AG_NEWS\n",
        "\n",
        "train_iter = iter(AG_NEWS(split=\"train\"))\n",
        "next(train_iter)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSEW-6cS1geM"
      },
      "source": [
        "In this step, we will take advantage of PyTorch's integrated tokenizers, specifically the \"**basic_english**\" tokenizer.   \n",
        "The code provided below will tokenize our text data and establish the necessary vocabulary required for our embedding layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXaky6RN1IXE"
      },
      "outputs": [],
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "train_iter = AG_NEWS(split=\"train\")\n",
        "\n",
        "\n",
        "def yield_tokens(data_iter):\n",
        "    for _, text in data_iter:\n",
        "        yield tokenizer(text)\n",
        "\n",
        "\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
        "vocab.set_default_index(vocab[\"<unk>\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9aNddP_95OZ"
      },
      "source": [
        "We will also load the GloVe embeddings for later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ylny-yyLMR6C"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def load_glove_embeddings(path, vocab, embedding_dim):\n",
        "    # Load GloVe embeddings into a dictionary\n",
        "    glove_embeddings = {}\n",
        "    with open(path, 'r', encoding='utf8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            vector = torch.tensor([float(val) for val in values[1:]], dtype=torch.float32)\n",
        "            glove_embeddings[word] = vector\n",
        "\n",
        "    # Create a weights matrix for words in vocab\n",
        "    weights_matrix = torch.zeros((len(vocab), embedding_dim))\n",
        "    for word, idx in vocab.get_stoi().items():\n",
        "        if word in glove_embeddings:\n",
        "            weights_matrix[idx] = glove_embeddings[word]\n",
        "        else:\n",
        "            weights_matrix[idx] = torch.randn(embedding_dim)  # or torch.zeros(embedding_dim)\n",
        "\n",
        "    return weights_matrix\n",
        "\n",
        "embedding_dim = 100\n",
        "glove_path = \"glove.6B.100d.txt\"\n",
        "weights_matrix = load_glove_embeddings(glove_path, vocab, embedding_dim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alr71yVLMTLX"
      },
      "source": [
        "The following method converts a text in tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4R3Rd5i3H95"
      },
      "outputs": [],
      "source": [
        "def process_text(text):\n",
        "    return torch.tensor(vocab(tokenizer(text)), dtype=torch.int64)\n",
        "process_text('here is the an example')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrXNZ1KS3Z4J"
      },
      "source": [
        "__Creating a Custom DataLoader for Texts__\n",
        "\n",
        "In this section, we will set up a custom DataLoader for our text data. This involves some advanced PyTorch techniques. If you prefer to skip the details and proceed directly, you can safely run the next cell, which provides a custom DataLoader. However, if you're interested in understanding what's happening under the hood, here's what we're doing:\n",
        "\n",
        "**Understanding `collate_fn` in DataLoader:**\n",
        "\n",
        "Before your data is fed into your model for training or inference, it goes through a crucial function called `collate_fn`. Here's what you need to know about it:\n",
        "\n",
        "- **Batch Processing**: DataLoader groups your data into batches, where each batch contains a specified number of data samples. The input to `collate_fn` is one of these batches.\n",
        "\n",
        "- **Data Processing Pipelines**: Prior to using DataLoader, you typically create data processing pipelines to convert your raw data into a format suitable for your model. `collate_fn` is responsible for applying these transformations to each batch of data.\n",
        "\n",
        "- **Top-Level Function**: It's essential to define `collate_fn` as a top-level Python function, not nested within other functions. This ensures that the function is accessible to all worker processes, which is crucial for parallel processing.\n",
        "\n",
        "Additionally, as we create our custom DataLoader, we will keep track of the length of each text. This information will be useful later when we use an RNN (Recurrent Neural Network) for processing sequences of varying lengths.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ltOXLgkL6NbX"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Collate Function for DataLoader\n",
        "def collate_batch(batch):\n",
        "    label_list, text_list, lengths = [], [], []\n",
        "    for (_label, _text) in batch:\n",
        "         label_list.append(_label - 1)  # Adjusting label to 0-based index\n",
        "         processed_text = process_text(_text)\n",
        "         text_list.append(processed_text)\n",
        "         lengths.append(processed_text.size(0))\n",
        "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
        "    lengths = torch.tensor(lengths, dtype=torch.int64)\n",
        "    text_list = torch.nn.utils.rnn.pad_sequence(text_list, batch_first=True)\n",
        "    return text_list.to(device), label_list.to(device), lengths.to(device)\n",
        "\n",
        "train_iter, test_iter = AG_NEWS(split=('train', 'test'))\n",
        "train_loader = DataLoader(train_iter, batch_size=8, shuffle=True, collate_fn=collate_batch)\n",
        "test_loader = DataLoader(test_iter, batch_size=8, shuffle=True, collate_fn=collate_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o60AaZMtHQ3Z"
      },
      "source": [
        "# Designing Our Neural Network Classifier\n",
        "\n",
        "Now, let's design our neural network classifier for text classification. The model consists of several key components:\n",
        "\n",
        "1. **Embedding Layer**: The initial layer of the model is an embedding layer that converts tokens into learnable embeddings. These embeddings capture the semantic meaning of words in our text data.  \n",
        "Here we will let the possibility to use pre-trained GloVe representations without modifying them to compare the performance obtained with custom embeddings or GloVe.\n",
        "\n",
        "2. **Mean Pooling**: Similar to our previous approach with Gensim, we will compute the mean of all the token embeddings within the text to create a global text embedding. This global representation captures the overall content of the text.\n",
        "\n",
        "3. **Linear Layer**: A linear layer will follow, which takes the global text embedding as input and predicts the class label according to this representation.\n",
        "\n",
        "Complete the following code to create the model described above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjL0Rq_s398w"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "\n",
        "class TextClassificationModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_class, use_pre_trained=False):\n",
        "        super(TextClassificationModel, self).__init__()\n",
        "        if use_pre_trained:\n",
        "          self.embedding = nn.Embedding.from_pretrained(weights_matrix, freeze=True)\n",
        "        else:\n",
        "          self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "        # self.embedding.weight.requires_grad = False\n",
        "        self.fc = nn.Linear(embed_dim, num_class)\n",
        "\n",
        "    def get_embeddings(self, text):\n",
        "        embedded = self.embedding(text)\n",
        "        return torch.mean(embedded, dim=1) # compute the mean of the embeddings\n",
        "\n",
        "    def forward(self, text):\n",
        "        embedded = self.get_embeddings(text)\n",
        "        return self.fc(embedded) # compute the logits i.e. the output of the linear layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6Gd6wMGJov_"
      },
      "source": [
        "Complete the following training and testing functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJPkyNYk4R6A"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "\n",
        "def train(model, dataloader, optimizer, criterion, epochs=5):\n",
        "  model.train()\n",
        "  for epoch in range(epochs):\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    total = 0\n",
        "    t = tqdm(dataloader)\n",
        "    for i, (text, labels, _) in enumerate(t):\n",
        "\n",
        "      pred = model(text)\n",
        "      loss = criterion(pred, labels)\n",
        "\n",
        "      _, predicted = pred.max(1)\n",
        "      running_corrects += predicted.eq(labels).sum().item()\n",
        "      total += labels.size(0)\n",
        "      running_loss += loss.item()\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      t.set_description(f\"epoch:{epoch} loss: {(running_loss / (i+1)):.4f} current accuracy:{round(running_corrects / total * 100, 2)}%\")\n",
        "\n",
        "def test(model, dataloader):\n",
        "    model.eval()\n",
        "    test_corrects = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for text, labels, _ in dataloader:\n",
        "            pred = model(text)\n",
        "            _, predicted = pred.max(1)\n",
        "            test_corrects += predicted.eq(labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    return test_corrects / total"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYl5Rtk5O2NB"
      },
      "source": [
        "Now compare the performance of this neural classifier when using pre-trained GloVe embeddings or embeddings computed from scratch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t4h9YfDE5Ii-"
      },
      "outputs": [],
      "source": [
        "LR = 0.005\n",
        "num_class = 4\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 100\n",
        "model = TextClassificationModel(vocab_size, embedding_dim, num_class).to(device)\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "train(model, train_loader, optimizer, criterion) # train the model for 5 or 10 epochs\n",
        "test(model, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPgt-VqK3TwV"
      },
      "outputs": [],
      "source": [
        "# model with pre-trained embeddings\n",
        "model = TextClassificationModel(vocab_size, embedding_dim, num_class, True).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "train(model, train_loader, optimizer, criterion)\n",
        "test(model, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DRJ9nSYU_dc"
      },
      "source": [
        "On the test set, get the embeddings of each text and plot them using a t-SNE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11YzAkodVqgD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "embeddings, labels = [], []\n",
        "for texts, label, _ in test_loader:\n",
        "    with torch.no_grad(): # we don't need to compute the gradients\n",
        "        em = model(texts).cuda()\n",
        "        embeddings.append(em.cpu().numpy()) # you need to send the embeddings to the cpu and convert them to numpy: .cpu().numpy()\n",
        "        labels.extend(label.cpu()) # same here\n",
        "\n",
        "embeddings = np.vstack(embeddings)\n",
        "labels = np.array(labels)\n",
        "\n",
        "X_tsne = TSNE(n_components=2, random_state=0).fit_transform(embeddings)\n",
        "\n",
        "plot_data = pd.DataFrame({'TSNE1': X_tsne[:, 0], 'TSNE2': X_tsne[:, 1], 'Label': labels})\n",
        "plt.figure(figsize=(16, 10))\n",
        "sns.scatterplot(x=\"TSNE1\", y=\"TSNE2\", hue=\"Label\", palette=sns.color_palette(\"hls\", len(np.unique(labels))), data=plot_data, legend=\"full\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0_llg-bWinY"
      },
      "source": [
        "# Adding Recurrence with LSTM Layers\n",
        "\n",
        "To enhance our network's capacity for understanding sequential data, we will incorporate [LSTM layers](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html) into our model. The following code snippet implements a recurrent neural network (RNN) using LSTM layers.\n",
        "\n",
        "A subtlety in this implementation lies in the use of the `pack_padded_sequence` method. This method is employed to handle sequences of varying lengths. Since not all sequences within a batch have the same length, we need to create \"fake\" empty tokens to ensure that all elements within a batch have the same dimension. This is precisely what the `pack_padded_sequence` method accomplishes.\n",
        "\n",
        "Additionally, we'll use the last hidden representation of the LSTM as the embedding for the global text representation.\n",
        "\n",
        "Furthermore, it's crucial to note that, since we need to send the lengths of the texts to the model, we will have to include this information within the `forward` method. This modification will also necessitate adjustments in our training and testing methods.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZsLDwJp37McR"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class RecurrentTextClassifier(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size, embedding_dim, output_dim=4):\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim, sparse=True)\n",
        "    self.lstm = nn.LSTM(embedding_dim,\n",
        "                        100,\n",
        "                        num_layers=2,\n",
        "                        dropout=0.2,\n",
        "                        batch_first=True)\n",
        "    self.fc = nn.Linear(100, output_dim)\n",
        "\n",
        "  def forward(self, text, text_lengths):\n",
        "      embedded = self.get_embeddings(text, text_lengths)\n",
        "      outputs=self.fc(embedded)\n",
        "      return outputs\n",
        "\n",
        "\n",
        "  def get_embeddings(self, text, text_lengths):\n",
        "      embedded = self.embedding(text)\n",
        "      #packed sequence\n",
        "      packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "      _, (hidden, cell) = self.lstm(packed_embedded)\n",
        "      return hidden[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpiSjCcBIS3I"
      },
      "outputs": [],
      "source": [
        "def train(model, dataloader, optimizer, criterion, epochs=5):\n",
        "  model.train()\n",
        "  for epoch in range(epochs):\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    total = 0\n",
        "    t = tqdm(dataloader)\n",
        "    for i, (text, labels, text_lengths) in enumerate(t):\n",
        "\n",
        "      pred = model(text, text_lengths).squeeze() #convert to 1D tensor\n",
        "      loss = criterion(pred, labels)\n",
        "\n",
        "      _, predicted = pred.max(1)\n",
        "      running_corrects += predicted.eq(labels).sum().item()\n",
        "      total += labels.size(0)\n",
        "      running_loss += loss.item()\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      t.set_description(f\"epoch:{epoch} loss: {(running_loss / (i+1)):.4f} current accuracy:{round(running_corrects / total * 100, 2)}%\")\n",
        "\n",
        "def test(model, dataloader):\n",
        "  model.eval()\n",
        "  test_corrects = 0\n",
        "  total = 0\n",
        "  with torch.no_grad():\n",
        "      for text, labels, text_lengths in dataloader:\n",
        "          pred = model(text, text_lengths).squeeze()\n",
        "          _, predicted = pred.max(1)\n",
        "          test_corrects += predicted.eq(labels).sum().item()\n",
        "          total += labels.size(0)\n",
        "  return test_corrects / total"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OiLjAtQXOa4"
      },
      "source": [
        "Train a reccurent model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MwtTfglMWo2F"
      },
      "outputs": [],
      "source": [
        "model = RecurrentTextClassifier(vocab_size, embedding_dim).cuda()\n",
        "criterion = criterion\n",
        "optimizer = torch.optim.SGD(model.parameters(), LR)\n",
        "train(model, train_loader, optimizer, criterion)\n",
        "test(model, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZXM3i49XUJ0"
      },
      "source": [
        "Plot the corresponding representatons on a t-SNE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zS9dxR9XTXL"
      },
      "outputs": [],
      "source": [
        "embeddings, labels = [], []\n",
        "for texts, label, text_lengths in test_loader:\n",
        "    with torch.no_grad(): # we don't need to compute the gradients\n",
        "        em = model(texts, text_lengths).cuda()\n",
        "        embeddings.append(em.cpu().numpy()) # you need to send the embeddings to the cpu and convert them to numpy: .cpu().numpy()\n",
        "        labels.extend(label.cpu()) # same here\n",
        "\n",
        "embeddings = np.vstack(embeddings)\n",
        "labels = np.array(labels)\n",
        "\n",
        "X_tsne = TSNE(n_components=2, random_state=0).fit_transform(embeddings)\n",
        "\n",
        "plot_data = pd.DataFrame({'TSNE1': X_tsne[:, 0], 'TSNE2': X_tsne[:, 1], 'Label': labels})\n",
        "plt.figure(figsize=(16, 10))\n",
        "sns.scatterplot(x=\"TSNE1\", y=\"TSNE2\", hue=\"Label\", palette=sns.color_palette(\"hls\", len(np.unique(labels))), data=plot_data, legend=\"full\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EN3RNwJ4Jbu_"
      },
      "source": [
        "# Transformers: Revolutionizing Natural Language Processing\n",
        "\n",
        "Transformers models have emerged as the cutting-edge technology in the field of natural language processing (NLP). They have significantly advanced the capabilities of NLP models, achieving state-of-the-art performance on various tasks.\n",
        "\n",
        "[Hugging Face](https://huggingface.co/) offers one of the most practical and comprehensive [libraries](https://huggingface.co/docs/transformers/main/en/index) for working with transformers and pre-trained models.\n",
        "\n",
        "**Hugging Face Transformers**: This library provides a user-friendly and powerful toolkit for working with transformers. It offers access to a wide range of pre-trained transformer models, making it easier than ever to leverage the latest advancements in NLP. With Hugging Face Transformers, you can quickly implement transformer-based solutions for tasks such as text classification, machine translation, and question-answering.\n",
        "\n",
        "Hugging Face Transformers has become an essential resource for NLP practitioners, researchers, and developers, streamlining the process of incorporating state-of-the-art transformer models into their projects.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80CEPIRFJ-O-"
      },
      "outputs": [],
      "source": [
        "%pip install transformers datasets 2>&1\n",
        "%pip install accelerate -U  2>&1\n",
        "%pip install transformers[torch]  2>&1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y67_ESTRfo1I"
      },
      "source": [
        "We will use the Hugging Face Transformers library to implement a transformer-based text classifier.  \n",
        "First, we will load our data and prepare it for use with torch and the transformers library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OjzWkv5SZUPB"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_df = pd.read_csv('train.csv', names=[\"label\", \"title\", \"text\"]).sample(40000)\n",
        "test_df = pd.read_csv('test.csv', names=[\"label\", \"title\", \"text\"]).sample(2000)\n",
        "train_text, train_labels = train_df[\"text\"], train_df['label']-1\n",
        "test_text, test_labels = test_df[\"text\"], test_df['label']-1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DeFnGObfo1I"
      },
      "source": [
        "We will use the `distilbert-base-uncased` model, which is a distilled version of the popular [BERT](https://arxiv.org/abs/1810.04805) model. Distilled models are smaller and faster than their full-size counterparts, making them ideal for applications with limited computational resources.  \n",
        "BERT uses a particular tokenization method called [WordPiece](https://huggingface.co/learn/nlp-course/chapter6/6?fw=pt), which is different from the tokenization method we used previously. Therefore, we will need to use the `DistilBertTokenizer` class to tokenize our data.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttbfElWgZ1z6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import DistilBertTokenizerFast\n",
        "\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "class NlpDataset(Dataset):\n",
        "    def __init__(self,data,labels,tokenizer):\n",
        "        self.data = data.to_list()\n",
        "        self.labels = labels.tolist()\n",
        "        self.encodings = tokenizer(self.data, truncation=True, padding=True)\n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx],dtype=torch.long)\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "\n",
        "train_dataset = NlpDataset(train_text, train_labels, tokenizer)\n",
        "test_dataset = NlpDataset(test_text,test_labels, tokenizer)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vwys9orUaegM"
      },
      "source": [
        "Look at a sample yielded by your train loader.  \n",
        "Its a dictionary with the following keys:\n",
        "- input_ids: the tokenized text\n",
        "- attention_mask: a mask to ignore padding\n",
        "- labels: the labels\n",
        "\n",
        "Input_ids are the tokenized text and labels are the labels.  \n",
        "The attention mask is a mask to ignore padding. Indeed, the model will pad the sequences to have the same length. The attention mask will tell the model to ignore the padding.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2_qT0CJNaYeo"
      },
      "outputs": [],
      "source": [
        "next(iter(train_loader))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hTZj2Tzai02"
      },
      "source": [
        "We will now instanciate our model and wrapp it into a Pytorch module.\n",
        "In this practical session we will freeze the model and only train the last layer.  \n",
        "This is not the best practice when using BERT but we will do it for the sake of simplicity.  \n",
        "Take a particular attention to the `forward` method.  \n",
        "The model will return a tuple with the logits, the hidden states and the attentions.  \n",
        "The logits are the outputs of the last layer.\n",
        "The hidden states are the outputs of all the layers and will contain the embeddings of the text.\n",
        "The attentions are the attention weights of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqMoeoTvajS3"
      },
      "outputs": [],
      "source": [
        "from transformers import  DistilBertForSequenceClassification\n",
        "import torch.nn as nn\n",
        "\n",
        "class BertClf(nn.Module):\n",
        "\n",
        "    def __init__(self, distilbert):\n",
        "\n",
        "        super(BertClf, self).__init__()\n",
        "\n",
        "        self.distilbert = distilbert\n",
        "        for name, param in distilbert.named_parameters():\n",
        "            if not \"classifier\" in name:\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def forward(self, sent_id, mask):\n",
        "\n",
        "        out = self.distilbert(sent_id, attention_mask=mask)\n",
        "        logits = out.logits\n",
        "        attn = out.attentions\n",
        "        hidden_states = out.hidden_states\n",
        "\n",
        "\n",
        "        return logits,hidden_states,attn\n",
        "\n",
        "distilbert = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\",\n",
        "                                                                  num_labels=4,\n",
        "                                                                  output_attentions=True,\n",
        "                                                                  output_hidden_states=True)\n",
        "\n",
        "model = BertClf(distilbert)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2ccG95Lfo1J"
      },
      "source": [
        "Now complete the following training and testing functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8C5DBKHalES"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "\n",
        "def train_bert(model, optimizer, dataloader, epochs):\n",
        "  model.train()\n",
        "  for epoch in range(epochs):\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    total = 0\n",
        "    t = tqdm(dataloader)\n",
        "    for i, batch in enumerate(t):\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "        preds, _, _ = model(input_ids, attention_mask)\n",
        "        loss = criterion(preds, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        _, predicted = preds.max(1)\n",
        "        running_corrects += predicted.eq(labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        t.set_description(f\"epoch:{epoch} loss: {(running_loss / (i+1)):.4f} current accuracy:{round(running_corrects / total * 100, 2)}%\")\n",
        "\n",
        "def test_bert(model, dataloader):\n",
        "    model.eval()\n",
        "    test_corrects = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "      for batch in tqdm(dataloader):\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        preds, _, _ = model(input_ids, attention_mask)\n",
        "        _, predicted = preds.max(1)\n",
        "        test_corrects += predicted.eq(labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "    return test_corrects / total"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIS0fLsqfo1J"
      },
      "source": [
        "We will now compare the performance of our transformer-based model with the previous models.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yyFyfiPkavkl"
      },
      "outputs": [],
      "source": [
        "from transformers import AdamW\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "optimizer = AdamW(model.parameters(),lr = 1e-5)\n",
        "criterion  = criterion\n",
        "n_epochs = 1\n",
        "\n",
        "train_bert(model, optimizer, train_loader, n_epochs)\n",
        "test_bert(model, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1sttc8Gfo1J"
      },
      "source": [
        "The following function get the embeddings of each text.\n",
        "BERT use a speacial token `[CLS]` to represent the whole text.  \n",
        "We will use the hidden states of this token as the embeddings of the text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YKqrqB5taysS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "test_loader = DataLoader(test_dataset, batch_size=1)\n",
        "\n",
        "\n",
        "def get_embeddings(model, dataloader):\n",
        "    model.eval()\n",
        "    embeddings = []\n",
        "    labels = []\n",
        "    with torch.no_grad():\n",
        "      for batch in tqdm(dataloader):\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels.append(batch[\"labels\"].item())\n",
        "\n",
        "        _, emb, _ = model(input_ids,mask=attention_mask)\n",
        "        last_layer_cls = emb[-1][:,0,:]\n",
        "        embeddings.append(last_layer_cls.squeeze(0).squeeze(0))\n",
        "    embeddings = np.array([e.cpu().numpy() for e in embeddings])\n",
        "    return embeddings, labels\n",
        "\n",
        "embeddings, labels = get_embeddings(model, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGEpluLcfo1J"
      },
      "source": [
        "Now plot the embeddings of each text on a t-SNE.  \n",
        "What do you think of these representations?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9S2MG7fPa08B"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "import seaborn as sns\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UMVpZlGa5uW"
      },
      "source": [
        "We can use[ **bertviz** library](https://github.com/jessevig/bertviz) to visualize the attention weights of the model.\n",
        "Remember that the attention weights are the weights of the attention mechanism of the model.\n",
        "It is a mechanism that allows the model to focus on the most important words of the text.\n",
        "Attention weights are very useful to understand how the model works and what it focuses on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EzPSGjYba7gq"
      },
      "outputs": [],
      "source": [
        "!pip install bertviz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgHOCf1Dfo1K"
      },
      "source": [
        "Here is a text example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrxAxUmUa_C5"
      },
      "outputs": [],
      "source": [
        "from bertviz import model_view, head_view\n",
        "\n",
        "sentence = test_df[\"text\"].iloc[33]\n",
        "tokenized = tokenizer(sentence)\n",
        "print(sentence)\n",
        "print(tokenized)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xO99Rbmfo1K"
      },
      "source": [
        "We will use the `model_view` function to visualize the attention weights of the model.  \n",
        "Here, each row represents a layer of the model and each column represents a head of the model.\n",
        "The darker the color, the higher the attention weight.  \n",
        "You may observe that some attention heads catche different relations between words.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8pHvDn5bAdQ"
      },
      "outputs": [],
      "source": [
        "inputs = torch.tensor(tokenized[\"input_ids\"]).unsqueeze(0).to(device)\n",
        "mask = torch.tensor(tokenized[\"attention_mask\"]).unsqueeze(0).to(device)\n",
        "outputs = model(inputs,mask = mask)\n",
        "attention = outputs[-1]\n",
        "tokens = tokenizer.convert_ids_to_tokens(inputs[0])\n",
        "model_view(attention, tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_VN6ycUfo1K"
      },
      "source": [
        "The ```head_view``` function allows to visualize the attention weights of a particular head of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QR6q_L8HbCxd"
      },
      "outputs": [],
      "source": [
        "head_view(attention, tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QvlvAGffo1K"
      },
      "source": [
        "That's it for this practical session.\n",
        "By now, you should have a good understanding of how to encode text data and use it to train a machine learning model.  \n",
        "You should also be familiar with the latest advancements in NLP, including transformers and pre-trained models.   \n",
        "We hope you enjoyed this practical session and that you will be able to apply what you've learned to your own projects."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.6.9 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
