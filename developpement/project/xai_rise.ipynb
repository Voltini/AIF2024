{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "VAf15cKpOiTd",
   "metadata": {
    "id": "VAf15cKpOiTd"
   },
   "source": [
    "Téléchargement base de données + préprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6819959b",
   "metadata": {
    "id": "6819959b"
   },
   "source": [
    "Pour Linux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xbozjPNqcj_U",
   "metadata": {
    "id": "xbozjPNqcj_U"
   },
   "outputs": [],
   "source": [
    "!wget https://s3.amazonaws.com/fast-ai-imageclas/imagenette2.tgz > /dev/null 2>&1\n",
    "!tar zxvf imagenette2.tgz > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f7054b",
   "metadata": {
    "id": "09f7054b"
   },
   "source": [
    "Pour MacOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86064f8a",
   "metadata": {
    "id": "86064f8a"
   },
   "outputs": [],
   "source": [
    "# test if file exists\n",
    "import os\n",
    "if not os.path.exists('imagenette2.tgz'):\n",
    "    !curl -O -# https://s3.amazonaws.com/fast-ai-imageclas/imagenette2.tgz\n",
    "if not os.path.exists('imagenette2'):\n",
    "    !tar -zxvf imagenette2.tgz > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "M5gVifREq6Vj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M5gVifREq6Vj",
    "outputId": "64d91cdb-1569-4150-ce7e-0790ae79f33d"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e64b679-4574-4872-a372-bdcf2e8b5e77",
   "metadata": {
    "id": "6e64b679-4574-4872-a372-bdcf2e8b5e77"
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "means, stds = (0.485, 0.456, 0.406), (0.229, 0.224, 0.225)\n",
    "train_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(means, stds),\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(means, stds),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def get_imagenette2_loaders(root_path=\"./imagenette2\", **kwargs):\n",
    "\n",
    "    trainset = torchvision.datasets.ImageFolder(\n",
    "        os.path.join(root_path, \"train\"), transform=train_transform\n",
    "    )\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, **kwargs)\n",
    "    testset = torchvision.datasets.ImageFolder(\n",
    "        os.path.join(root_path, \"val\"), transform=test_transform\n",
    "    )\n",
    "    testloader = torch.utils.data.DataLoader(testset, **kwargs)\n",
    "    return trainloader, testloader\n",
    "\n",
    "\n",
    "trainloader, testloader = get_imagenette2_loaders(\n",
    "    batch_size=64, shuffle=True, num_workers=2\n",
    ")\n",
    "\n",
    "labels = [\n",
    "    \"tench\",\n",
    "    \"English springer\",\n",
    "    \"cassette player\",\n",
    "    \"chain saw\",\n",
    "    \"church\",\n",
    "    \"French horn\",\n",
    "    \"garbage truck\",\n",
    "    \"gas pump\",\n",
    "    \"golf ball\",\n",
    "    \"parachute\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20b7fa6-d651-4839-bda2-0657f3ff3c54",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 378
    },
    "id": "c20b7fa6-d651-4839-bda2-0657f3ff3c54",
    "outputId": "fd4bdc47-ef9a-4a8e-f8f5-812019954944"
   },
   "outputs": [],
   "source": [
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "inv_normalize = transforms.Normalize(\n",
    "    mean=[-m / s for m, s in zip(means, stds)], std=[1 / s for s in stds]\n",
    ")\n",
    "\n",
    "x, y = next(iter(trainloader))\n",
    "img_grid = make_grid(x[:16])\n",
    "img_grid = inv_normalize(img_grid)\n",
    "plt.figure(figsize=(20, 15))\n",
    "plt.imshow(img_grid.permute(1, 2, 0))\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-Rn3DquJO74T",
   "metadata": {
    "id": "-Rn3DquJO74T"
   },
   "source": [
    "Modèle 1 : pr-trained VGG11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "myxoTJF2PEIF",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "myxoTJF2PEIF",
    "outputId": "14779592-fcec-4e1e-b455-34672b5a94b9"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "model_vgg11 = torchvision.models.vgg11(pretrained=True)\n",
    "for param in model_vgg11.features:\n",
    "    param.requires_grad = False\n",
    "\n",
    "model_vgg11.classifier = nn.Sequential(\n",
    "    nn.Linear(in_features=25088, out_features=4096, bias=True),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(in_features=4096, out_features=4096, bias=True),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(in_features=4096, out_features=10, bias=True),\n",
    ")\n",
    "if torch.cuda.is_available():\n",
    "    model_vgg11 = model_vgg11.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OiqIQWnop6gW",
   "metadata": {
    "id": "OiqIQWnop6gW"
   },
   "source": [
    "Modèle 2 : Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TEOcp3uCsWhu",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TEOcp3uCsWhu",
    "outputId": "51493fa1-1157-4eac-ac80-022d12a001e7"
   },
   "outputs": [],
   "source": [
    "model_resnet18 = torchvision.models.resnet18(pretrained=True)\n",
    "for param in model_resnet18.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model_resnet18.fc = nn.Linear(model_resnet18.fc.in_features, 10)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model_resnet18 = model_resnet18.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yUZPoYQntXU-",
   "metadata": {
    "id": "yUZPoYQntXU-"
   },
   "source": [
    "Entraînement, test et évaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QPFtkuztPtUD",
   "metadata": {
    "id": "QPFtkuztPtUD"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "criterion_classifier = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "\n",
    "\n",
    "def train(model, optimizer, trainloader, epochs=30):\n",
    "    t = tqdm(range(epochs))\n",
    "    for epoch in t:\n",
    "        corrects = 0\n",
    "        total = 0\n",
    "        for x, y in trainloader:\n",
    "            loss = 0\n",
    "            if torch.cuda.is_available():\n",
    "                x = x.cuda()\n",
    "                y = y.cuda()\n",
    "            y_hat = model(x)\n",
    "\n",
    "            loss += criterion_classifier(y_hat, y)\n",
    "            _, predicted = y_hat.max(1)\n",
    "            corrects += predicted.eq(y).sum().item()\n",
    "            total += y.size(0)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            t.set_description(\n",
    "                f\"epoch: {epoch}; current accuracy: {round(corrects / total * 100, 2)}%  \"\n",
    "            )\n",
    "    return corrects / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "K2aupdyiPwU8",
   "metadata": {
    "id": "K2aupdyiPwU8"
   },
   "outputs": [],
   "source": [
    "learning_rate = 5e-3\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_hJpMEUTt41Q",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_hJpMEUTt41Q",
    "outputId": "3ee219f5-f863-4b39-a667-37d8f71ba6a4"
   },
   "outputs": [],
   "source": [
    "# vgg11\n",
    "optimizer = torch.optim.Adam(model_vgg11.classifier.parameters(), lr=learning_rate)\n",
    "train(model_vgg11, optimizer, trainloader, epochs=epochs)\n",
    "\n",
    "# resnet18\n",
    "optimizer = torch.optim.Adam(model_resnet18.fc.parameters(), lr=learning_rate)\n",
    "train(model_resnet18, optimizer, trainloader, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mjuGr0NFPyED",
   "metadata": {
    "id": "mjuGr0NFPyED"
   },
   "outputs": [],
   "source": [
    "def test(model, dataloader):\n",
    "    test_corrects = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            if torch.cuda.is_available():\n",
    "                x = x.cuda()\n",
    "                y = y.cuda()\n",
    "            y_hat = model(x).argmax(1)\n",
    "            test_corrects += y_hat.eq(y).sum().item()\n",
    "            total += y.size(0)\n",
    "    return test_corrects / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "X0PMVEpNqAqL",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0PMVEpNqAqL",
    "outputId": "60652fdd-38c6-4d00-e2cf-d2a31500dfeb"
   },
   "outputs": [],
   "source": [
    "model_vgg11.eval()\n",
    "test_acc = test(model_vgg11, testloader) * 100\n",
    "print(f\"Test accuracy vgg11: {test_acc:.2f} %\")\n",
    "\n",
    "model_resnet18.eval()\n",
    "test_acc = test(model_resnet18, testloader) * 100\n",
    "print(f\"Test accuracy resnet18: {test_acc:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "n8LClgDeVp1T",
   "metadata": {
    "id": "n8LClgDeVp1T"
   },
   "source": [
    "Méthode 1 : RISE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lqDXwXC2Vn8x",
   "metadata": {
    "id": "lqDXwXC2Vn8x"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from skimage.transform import resize\n",
    "\n",
    "\n",
    "def generate_masks(N, s, p1, image_size):\n",
    "    cell_size = np.ceil(np.array(image_size) / s)\n",
    "    up_size = (s + 1) * cell_size\n",
    "\n",
    "    grid = np.random.rand(N, s, s) < p1\n",
    "    grid = grid.astype(\"float32\")\n",
    "\n",
    "    masks = np.empty((N, *image_size))\n",
    "\n",
    "    for i in range(N):\n",
    "        # Random shifts\n",
    "        x = np.random.randint(0, cell_size[0])\n",
    "        y = np.random.randint(0, cell_size[1])\n",
    "        # Linear interpolation\n",
    "        masks[i, :, :] = resize(\n",
    "            grid[i], up_size, order=1, mode=\"reflect\", anti_aliasing=False\n",
    "        )[x : x + image_size[0], y : y + image_size[1]]\n",
    "\n",
    "    masks = masks.reshape(-1, 1, *image_size)\n",
    "    masks = torch.from_numpy(masks).float()\n",
    "\n",
    "    return masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "H3rN-MLkXLlD",
   "metadata": {
    "id": "H3rN-MLkXLlD"
   },
   "outputs": [],
   "source": [
    "def explain(model, N, p1, img, masks):\n",
    "    img = img.unsqueeze(0).to(\"cpu\")\n",
    "    _, _, H, W = img.size()\n",
    "    print(type(img), type(masks))\n",
    "    stack = torch.mul(masks, img)\n",
    "\n",
    "    p = []\n",
    "\n",
    "    for i in tqdm(range(N)):\n",
    "        with torch.no_grad():\n",
    "            input = stack[i : i + 1]\n",
    "            if torch.cuda.is_available():\n",
    "                input = input.cuda()\n",
    "            output = model(input)\n",
    "            p.append(output.to(\"cpu\"))\n",
    "\n",
    "    p = torch.cat(p)\n",
    "\n",
    "    CL = p.size(1)\n",
    "    sal = torch.matmul(p.data.transpose(0, 1), masks.view(N, H * W))\n",
    "    sal = sal.view((CL, H, W))\n",
    "    sal = sal / N / p1\n",
    "    return sal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LlhlvQpJYcji",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "id": "LlhlvQpJYcji",
    "outputId": "3b2ac6cd-07f6-419f-efd7-ac2a94df200b"
   },
   "outputs": [],
   "source": [
    "# Visualisation de la importance map pour la prédiction des modèles sur l'image suivante\n",
    "\n",
    "idx = 0\n",
    "\n",
    "img = inv_normalize(x[idx])\n",
    "np_img = np.transpose(img.cpu().detach().numpy(), (1, 2, 0)) * 255\n",
    "np_img = np_img.astype(np.uint8)\n",
    "plt.imshow(np_img)\n",
    "plt.axis(\"off\")\n",
    "input = x[idx].unsqueeze(0)\n",
    "if torch.cuda.is_available():\n",
    "    input = input.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MokdT2vqyte3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MokdT2vqyte3",
    "outputId": "d2435761-824d-43f4-c036-23be694ac367"
   },
   "outputs": [],
   "source": [
    "output = model_vgg11(input)\n",
    "_, prediction_vgg11 = torch.topk(output, 1)\n",
    "print(f\"VGG11 prediction: {labels[prediction_vgg11.item()]} (item number {prediction_vgg11.item()})\")\n",
    "\n",
    "output = model_resnet18(input)\n",
    "_, prediction_resnet18 = torch.topk(output, 1)\n",
    "print(f\"Resnet18 prediction: {labels[prediction_resnet18.item()]} (item number {prediction_resnet18.item()})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Fh6Lf_5QZ0SW",
   "metadata": {
    "id": "Fh6Lf_5QZ0SW"
   },
   "outputs": [],
   "source": [
    "N = 10000  # Number of masks\n",
    "s = 8  # Size of grid\n",
    "p1 = 0.1  # Probability of inclusion\n",
    "\n",
    "masks = generate_masks(N, s, p1, img.shape[-2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FSj3_10graIM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FSj3_10graIM",
    "outputId": "cdc737d0-4d02-4ec7-9d46-238ad4904a63"
   },
   "outputs": [],
   "source": [
    "saliency_maps_vgg11 = explain(model_vgg11, N, p1, img, masks)\n",
    "saliency_maps_resnet18 = explain(model_resnet18, N, p1, img, masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uD-QTtF0iwES",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 434
    },
    "id": "uD-QTtF0iwES",
    "outputId": "ff6cb2cf-d2b8-4f1a-c3cd-2ac1a73225e2"
   },
   "outputs": [],
   "source": [
    "# vgg11\n",
    "plt.imshow(np_img)\n",
    "\n",
    "plt.imshow(saliency_maps_vgg11[prediction_vgg11.item()], cmap=\"turbo\", alpha=0.5)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gi6ouzip0jxV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 434
    },
    "id": "gi6ouzip0jxV",
    "outputId": "8f3588d0-424e-4108-f974-e444f0ff484a"
   },
   "outputs": [],
   "source": [
    "# resnet18\n",
    "plt.imshow(np_img)\n",
    "\n",
    "plt.imshow(saliency_maps_resnet18[prediction_resnet18.item()], cmap=\"turbo\", alpha=0.5)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SAgSttaV1KGS",
   "metadata": {
    "id": "SAgSttaV1KGS"
   },
   "source": [
    "Méthode 2 : Vanilla gradient back-propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cn1jTqps6D",
   "metadata": {
    "id": "43cn1jTqps6D"
   },
   "outputs": [],
   "source": [
    "img = img.unsqueeze(0).cuda() # we need to set the input on GPU before the requires_grad operation!\n",
    "img.requires_grad_();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yGgTZtXLps8t",
   "metadata": {
    "id": "yGgTZtXLps8t"
   },
   "outputs": [],
   "source": [
    "# VGG11\n",
    "img.grad = None\n",
    "\n",
    "output = model_vgg11(img)\n",
    "output_idx = output.argmax()\n",
    "output_max = output[0, output_idx]\n",
    "\n",
    "output_max.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LXHze2dCps_G",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 550
    },
    "id": "LXHze2dCps_G",
    "outputId": "cdc873c3-9d34-41f9-9bee-ff03a9aef8e2"
   },
   "outputs": [],
   "source": [
    "saliency_vgg11, _ = torch.max(img.grad.data.abs(), dim=1)\n",
    "saliency_vgg11 = saliency_vgg11.squeeze(0)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(np_img)\n",
    "plt.axis('off')\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(saliency_vgg11.cpu(), cmap='hot')\n",
    "plt.axis('off')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mrLgKftWrJAm",
   "metadata": {
    "id": "mrLgKftWrJAm"
   },
   "outputs": [],
   "source": [
    "# Resnet18\n",
    "img.grad = None\n",
    "\n",
    "output = model_resnet18(img)\n",
    "output_idx = output.argmax()\n",
    "output_max = output[0, output_idx]\n",
    "\n",
    "output_max.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9nQ46glrQeB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 550
    },
    "id": "e9nQ46glrQeB",
    "outputId": "bcdb745f-d5e4-49cb-bc25-fd76c0f30063"
   },
   "outputs": [],
   "source": [
    "saliency_resnet18, _ = torch.max(img.grad.data.abs(), dim=1)\n",
    "saliency_resnet18 = saliency_resnet18.squeeze(0)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(np_img)\n",
    "plt.axis('off')\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(saliency_resnet18.cpu(), cmap='hot')\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FzblLbJ4mNeg",
   "metadata": {
    "id": "FzblLbJ4mNeg"
   },
   "source": [
    "Métriques d'évaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9nMy0QcnOCYQ",
   "metadata": {
    "id": "9nMy0QcnOCYQ"
   },
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "def deletion(model, img, saliency_map, target_class):\n",
    "    scores = []\n",
    "    modifiable_img = img.clone().detach()\n",
    "    modifiable_img = modifiable_img.squeeze(0)\n",
    "    C, H, W = modifiable_img.shape\n",
    "\n",
    "    # Créer un saliency_map appliqué à chaque canal\n",
    "    saliency_map = saliency_map.squeeze()\n",
    "    expanded_saliency_map = saliency_map.repeat(C, 1, 1)  # Répétition du saliency_map pour chaque canal\n",
    "\n",
    "    # Indices des pixels par ordre décroissant saliency\n",
    "    _, indices = torch.sort(expanded_saliency_map.view(-1), descending=True)\n",
    "\n",
    "    num_pixels_per_step = max(1, len(indices) // 100)  # 1% de l'image totale à chaque fois\n",
    "    num_steps = len(indices) // num_pixels_per_step\n",
    "\n",
    "    for step in range(num_steps):\n",
    "        if step == num_steps - 1:\n",
    "            indices_to_zero = indices[step * num_pixels_per_step:]  # Prendre tous les pixels restants si dernière itération\n",
    "        else:\n",
    "            indices_to_zero = indices[step * num_pixels_per_step:(step + 1) * num_pixels_per_step]\n",
    "\n",
    "        flat_img = modifiable_img.view(-1)\n",
    "        flat_img[indices_to_zero] = 0\n",
    "        modifiable_img = flat_img.view(C, H, W)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(modifiable_img.unsqueeze(0))\n",
    "            prob = F.softmax(output, dim=1)[0, target_class]\n",
    "            scores.append(prob.item())\n",
    "\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bUCARf8uri1",
   "metadata": {
    "id": "7bUCARf8uri1"
   },
   "outputs": [],
   "source": [
    "scores_vgg11_rise_deletion = deletion(model_vgg11, img, saliency_maps_vgg11[prediction_vgg11.item()], prediction_vgg11.item())\n",
    "scores_resnet18_rise_deletion = deletion(model_resnet18, img, saliency_maps_resnet18[prediction_resnet18.item()], prediction_resnet18.item())\n",
    "scores_vgg11_vanilla_deletion = deletion(model_vgg11, img, saliency_vgg11, prediction_vgg11.item())\n",
    "scores_resnet18_vanilla_deletion = deletion(model_resnet18, img, saliency_resnet18, prediction_resnet18.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZV2or3-O1yma",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 943
    },
    "id": "ZV2or3-O1yma",
    "outputId": "e156efe8-26cc-43d4-ac54-5a706b2aebc6"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(scores_vgg11_rise_deletion, color='blue')\n",
    "plt.title('VGG11 RISE Deletion')\n",
    "plt.xlabel('Pixel removed (%)')\n",
    "plt.ylabel('Model Confidence')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(scores_resnet18_rise_deletion, color='green')\n",
    "plt.title('ResNet18 RISE Deletion')\n",
    "plt.xlabel('Pixel removed (%)')\n",
    "plt.ylabel('Model Confidence')\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(scores_vgg11_vanilla_deletion, color='red')\n",
    "plt.title('VGG11 Vanilla Deletion')\n",
    "plt.xlabel('Pixel removed (%)')\n",
    "plt.ylabel('Model Confidence')\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(scores_resnet18_vanilla_deletion, color='purple')\n",
    "plt.title('ResNet18 Vanilla Deletion')\n",
    "plt.xlabel('Pixel removed (%)')\n",
    "plt.ylabel('Model Confidence')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pEjtnM1amXZk",
   "metadata": {
    "id": "pEjtnM1amXZk"
   },
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "def insertion(model, img, saliency_map, target_class):\n",
    "    scores = []\n",
    "    modifiable_img = torch.zeros_like(img).detach()  # Commencer avec une image nulle\n",
    "    modifiable_img = modifiable_img.squeeze(0)\n",
    "    C, H, W = modifiable_img.shape\n",
    "\n",
    "    # Créer un saliency_map appliqué à chaque canal\n",
    "    saliency_map = saliency_map.squeeze()\n",
    "    expanded_saliency_map = saliency_map.repeat(C, 1, 1)  # Répétition du saliency_map pour chaque canal\n",
    "\n",
    "    # Indices des pixels par ordre décroissant de saliency\n",
    "    _, indices = torch.sort(expanded_saliency_map.view(-1), descending=True)\n",
    "\n",
    "    num_pixels_per_step = max(1, len(indices) // 100)  # 1% de l'image totale à chaque fois\n",
    "    num_steps = len(indices) // num_pixels_per_step\n",
    "\n",
    "    original_img = img.clone().detach().squeeze(0).view(-1)  # L'image originale en format plat\n",
    "\n",
    "    for step in range(num_steps):\n",
    "        if step == num_steps - 1:\n",
    "            indices_to_add = indices[step * num_pixels_per_step:]  # Prendre tous les pixels restants si dernière itération\n",
    "        else:\n",
    "            indices_to_add = indices[step * num_pixels_per_step:(step + 1) * num_pixels_per_step]\n",
    "\n",
    "        flat_img = modifiable_img.view(-1)\n",
    "        flat_img[indices_to_add] = original_img[indices_to_add]  # Ajouter les pixels de l'image originale\n",
    "        modifiable_img = flat_img.view(C, H, W)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(modifiable_img.unsqueeze(0))\n",
    "            prob = F.softmax(output, dim=1)[0, target_class]\n",
    "            scores.append(prob.item())\n",
    "\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "h6bdHRp9mXb8",
   "metadata": {
    "id": "h6bdHRp9mXb8"
   },
   "outputs": [],
   "source": [
    "scores_vgg11_rise_insertion = insertion(model_vgg11, img, saliency_maps_vgg11[prediction_vgg11.item()], prediction_vgg11.item())\n",
    "scores_resnet18_rise_insertion = insertion(model_resnet18, img, saliency_maps_resnet18[prediction_resnet18.item()], prediction_resnet18.item())\n",
    "scores_vgg11_vanilla_insertion = insertion(model_vgg11, img, saliency_vgg11, prediction_vgg11.item())\n",
    "scores_resnet18_vanilla_insertion = insertion(model_resnet18, img, saliency_resnet18, prediction_resnet18.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xl9T9WFsdlm3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 943
    },
    "id": "xl9T9WFsdlm3",
    "outputId": "2d453577-1104-40b0-9ef2-00874b545898"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(scores_vgg11_rise_insertion, color='blue')\n",
    "plt.title('VGG11 RISE Insertion')\n",
    "plt.xlabel('Pixel added (%)')\n",
    "plt.ylabel('Model Confidence')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(scores_resnet18_rise_insertion, color='green')\n",
    "plt.title('ResNet18 RISE Insertion')\n",
    "plt.xlabel('Pixel added (%)')\n",
    "plt.ylabel('Model Confidence')\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(scores_vgg11_vanilla_insertion, color='red')\n",
    "plt.title('VGG11 Vanilla Insertion')\n",
    "plt.xlabel('Pixel added (%)')\n",
    "plt.ylabel('Model Confidence')\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(scores_resnet18_vanilla_insertion, color='purple')\n",
    "plt.title('ResNet18 Vanilla Insertion')\n",
    "plt.xlabel('Pixel added (%)')\n",
    "plt.ylabel('Model Confidence')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
